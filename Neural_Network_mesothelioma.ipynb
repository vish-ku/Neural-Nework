{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Network-2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFkAEm85_OFK"
      },
      "source": [
        "# Mesothelioma data set.\n",
        "\n",
        "We have to classify the if the patients have Mesothelioma or not. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7llsYDxb_iU"
      },
      "source": [
        "# importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import pickle"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxRKrekK4pAh"
      },
      "source": [
        "data = pd.read_excel(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00351/Mesothelioma%20data%20set.xlsx\")\n",
        "data = pd.get_dummies(data, columns = ['class of diagnosis'])\n",
        "data = data.sample(frac = 1).reset_index(drop = True) # shuffling the data set\n",
        "datainput = data.iloc[:,:-2] \n",
        "# normalizing the data, else the network shoots up too much that it returns Nan numbers!\n",
        "for column in datainput.columns:\n",
        "    datainput[column] = datainput[column]/datainput[column].max()\n",
        "dataoutput = data.iloc[:,-2:]\n",
        "# Doing a 75-25 % Training - Testing split.\n",
        "traininput = datainput.iloc[:240,:].values\n",
        "trainoutput = dataoutput.iloc[:240,:].values\n",
        "testinput = datainput.iloc[240:,:].values\n",
        "testoutput = dataoutput.iloc[240:,:].values\n"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tMahgrHg1NA"
      },
      "source": [
        "# These are all the activation functions that I have used.\n",
        "def relu(x):\n",
        "    if x > 0:\n",
        "        return x\n",
        "    else:\n",
        "        return 0\n",
        " \n",
        "def drelu(x):\n",
        "    if x > 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        " \n",
        "def sigmoid(x):\n",
        "    return 1/(1+math.exp(-x))\n",
        " \n",
        "def dsigmoid(x):\n",
        "    return sigmoid(x)*(1-sigmoid(x))\n",
        " \n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        " \n",
        "def dsoftmax(x):\n",
        "    return (x * np.identity(x.size)- x.transpose() @ x)"
      ],
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6pHPXjZhIuf"
      },
      "source": [
        "# I started by trying to modularize everything, but ended up writing a single function that, trains and tests\n",
        "# the loaded data. \n",
        "# I have introduced two more input parameters when compating with the mushroom jupyter notebook.\n",
        " \n",
        "# Parameters:\n",
        "# datainput - the training data\n",
        "# dataoutput - training data output\n",
        "# testinput - test data input\n",
        "# testoutput - test data output\n",
        "# layers - the number of layes in the network, this includes the number of hidden layers plus output layer.\n",
        "# node_per_layer - the number of nodes in a layer,\n",
        "# output - dimension of output/ number of classes. \n",
        "# batch_size - setting the batch size for learning and updating the weights.\n",
        "# learn_rate - the learning rate during backpropagation!\n",
        " \n",
        " \n",
        "def NeuralNet(datainput, dataoutput, testinput, testoutput, layers, node_per_layer, output,batch_size,learn_rate):\n",
        "\n",
        "    begin = time.time() # this is to calculate the runtime. \n",
        "    # Setting up all initial weights and biases, I have set them up as dictionaries\n",
        "    weights = {}\n",
        "    bias = {}\n",
        "    dweights = {}\n",
        "    dbias = {}\n",
        "    # These are set of dummy variables which I want in the shape of the weights, and their derivatives.\n",
        "    cweights = {}\n",
        "    cbias = {}\n",
        "    rweights = {}\n",
        "    rbias = {}\n",
        "    dim = len(datainput[0])\n",
        "    # Setting up initial weights and biases\n",
        "    for i in range(layers):\n",
        "        if i == 0:\n",
        "            weights[i] = np.random.normal(0,(2/dim)**(0.5),size = (node_per_layer,dim))\n",
        "            bias[i] = np.random.normal(0,1/2, size = (node_per_layer))\n",
        "            dweights[i] = np.zeros((node_per_layer,dim))\n",
        "            dbias[i] = np.zeros((node_per_layer))\n",
        "            cweights[i] = np.zeros((node_per_layer,dim))\n",
        "            cbias[i] = np.zeros((node_per_layer))\n",
        "            rweights[i] = np.zeros((node_per_layer,dim))\n",
        "            rbias[i] = np.zeros((node_per_layer))\n",
        "        elif i == layers-1:\n",
        "            weights[i] = np.random.normal(0,(2/dim)**(0.5),size = (output,node_per_layer))\n",
        "            bias[i] = np.random.normal(0,1/2,size = (output))\n",
        "            dweights[i] = np.random.uniform(1,5,size = (output,node_per_layer))\n",
        "            dbias[i] = np.random.uniform(1,4,size = output)\n",
        "            cweights[i] = np.zeros((output,node_per_layer))\n",
        "            cbias[i] = np.zeros((output))\n",
        "            rweights[i] = np.zeros((output,node_per_layer))\n",
        "            rbias[i] = np.zeros((output))\n",
        "        else:\n",
        "            weights[i] = np.random.normal(0,(2/dim)**(0.5),size = (node_per_layer,node_per_layer))\n",
        "            bias[i] = np.random.normal(0,1/2, size = (node_per_layer))\n",
        "            dweights[i] = np.random.uniform(1,5,size = (node_per_layer,node_per_layer))\n",
        "            dbias[i] = np.random.uniform(1,4,size = node_per_layer)\n",
        "            cweights[i] = np.zeros((node_per_layer,node_per_layer))\n",
        "            cbias[i] = np.zeros((node_per_layer))\n",
        "            rweights[i] = np.zeros((node_per_layer,node_per_layer))\n",
        "            rbias[i] = np.zeros((node_per_layer))\n",
        "    \n",
        " \n",
        "    a = {} #activation\n",
        "    z = {} #dummy to find activation using relu and softmax output.\n",
        "    delta = {} # catches the derivative of cost with respect to the dummy activation variables.\n",
        "    for i in range(layers):\n",
        "        if i == layers-1:\n",
        "            a[i] = np.empty(output)\n",
        "            z[i] = np.empty(output)\n",
        "            delta[i] = np.empty(output)\n",
        "        else:\n",
        "            a[i] = np.empty(node_per_layer)\n",
        "            z[i] = np.empty(node_per_layer)\n",
        "            delta[i] = np.empty(node_per_layer)\n",
        " \n",
        "    count = 0\n",
        "    # Iteration through the whole data set.\n",
        "    while count < 200:\n",
        "        flag = 0\n",
        "        itemcount = 0\n",
        "        for item in range(len(datainput)): # these are inputs from the data set.\n",
        " \n",
        "            # Calculating forwardpass with weights!\n",
        "            itemcount += 1\n",
        "            for i in range(layers):\n",
        "                if i == 0:\n",
        "                    for j in range(node_per_layer):\n",
        "                        z[i][j] = np.dot(datainput[item],weights[i][j]) + bias[i][j]\n",
        "                        a[i][j] = relu(z[i][j])\n",
        "                elif i == layers-1:\n",
        "                    for j in range(output):\n",
        "                        z[i][j] = np.dot(a[i-1],weights[i][j]) + bias[i][j]        \n",
        "                    a[i] = softmax(z[i])\n",
        "                else: \n",
        "                    for j in range(node_per_layer):\n",
        "                        z[i][j] = np.dot(a[i-1],weights[i][j]) + bias[i][j]\n",
        "                        a[i][j] = relu(z[i][j])\n",
        " \n",
        "            value = a[layers-1] #this is the forwardpass output.\n",
        "            dcost = np.empty((output)) \n",
        "            dvalue = dsoftmax(value) # the softmax deriative output\n",
        " \n",
        " \n",
        "            # Backpropagation! \n",
        " \n",
        "            # We use mean square error as our cost function. \n",
        "            # try to capture the derivatives in the same dictionary form as the weights, biases and the dummy z variables. \n",
        "            # Derivative of cost with respect to the forward pass output.\n",
        "            for i in range(output):\n",
        "                dcost[i] = -(dataoutput[item][i] - value[i])\n",
        " \n",
        "            # Finding the last layer gradients, all the gradient before this can be found from this. \n",
        "            for i in range(output):\n",
        "                delta[layers-1][i] = np.dot(dcost,dvalue[:,i]) # derivative with respect to the dummy z -variable\n",
        "                #delta[layers-1][i] = -2*(dataoutput[item][i]-value[i])*dsigmoid(z[layers-1][i])\n",
        "                dbias[layers-1][i] =  delta[layers-1][i]\n",
        "                for j in range(node_per_layer):\n",
        "                    dweights[layers-1][i][j] = delta[layers-1][i]*a[layers-2][j]\n",
        " \n",
        "            # Finding the gradient in the rest of the layers\n",
        "            for i in range(layers-1):\n",
        "                if i == layers-2:\n",
        "                    for j in range(node_per_layer):\n",
        "                        delta[layers-2-i][j] = np.dot(delta[layers-1-i],weights[layers-1-i][:,j])*drelu(z[layers-2-i][j])\n",
        "                        dbias[layers-2-i][j] = delta[layers-2-i][j]\n",
        "                        for k in range(dim):\n",
        "                            dweights[layers-2-i][j][k] = delta[layers-2-i][j] * datainput[item][k]\n",
        "                else:\n",
        "                    for j in range(node_per_layer):\n",
        "                        delta[layers-2-i][j] = np.dot(delta[layers-1-i],weights[layers-1-i][:,j])*drelu(z[layers-2-i][j])\n",
        "                        dbias[layers-2-i][j] = delta[layers-2-i][j]\n",
        "                        for k in range(node_per_layer):\n",
        "                            dweights[layers-2-i][j][k] = delta[layers-2-i][j]*a[layers-3-i][k]\n",
        " \n",
        " \n",
        "            # We train the model with batch size 20. i.e. find average gradient for 20 inputs and update the weights and biases\n",
        "            for i in range(layers):\n",
        "                cweights[i] = cweights[i] + dweights[i]\n",
        "                cbias[i] = cbias[i] + dbias[i]\n",
        "            \n",
        "            # updating weights in batches of size 20 - This was found to give meainingful results as compared to updating weights after and epoch.\n",
        "            # the 0.05 is the learning rate.\n",
        "            if flag > 0 and flag % batch_size == 0:\n",
        "                for i in range(layers):\n",
        "                    weights[i] = weights[i] -  (learn_rate) * cweights[i]/batch_size # the weights are being updated\n",
        "                    bias[i] = bias[i] -  (learn_rate) * cbias[i]/batch_size\n",
        "                    cweights[i] = rweights[i]\n",
        "                    cbias[i] = rbias[i]    \n",
        "            \n",
        "            flag += 1 \n",
        " \n",
        "            #if itemcount < 6 and count % 10 == 0:\n",
        "            #  print(value, trainoutput[item])\n",
        "        \n",
        " \n",
        "        count += 1 # counts epochs\n",
        "        if count % 40 == 0:\n",
        "            print(\"Iteration \", count, \"is now complete\")\n",
        "    end1 = time.time()\n",
        "    parameters = (weights, bias) # trained model weight!\n",
        "    pickle.dump(parameters, open('sample_data/parameters2.p', 'wb')) # dumping the trained model weights and biases\n",
        "    print(\"Training is now complete. It took\", round((end1-begin)/60,2) ,\"minutes to train the Network.\")\n",
        " \n",
        "    # Testing the model on untrained data.\n",
        "    # Now we test for accuracy.\n",
        "    correct = 0\n",
        "    outcounter = 0\n",
        "    for item in range(len(testinput)):\n",
        "        outcounter +=1\n",
        " \n",
        "        a = {} #activation\n",
        "        z = {} #dummy to find activation using relu.\n",
        "        for i in range(layers):\n",
        "            if i == layers-1:\n",
        "                a[i] = np.empty(output)\n",
        "                z[i] = np.empty(output)\n",
        "            else:\n",
        "                a[i] = np.empty(node_per_layer)\n",
        "                z[i] = np.empty(node_per_layer)\n",
        " \n",
        "        # Calculating forwardpass with weights!\n",
        "        for i in range(layers):\n",
        "            if i == 0:\n",
        "                for j in range(node_per_layer):\n",
        "                    z[i][j] = np.dot(testinput[item],weights[i][j]) + bias[i][j]\n",
        "                    a[i][j] = relu(z[i][j])\n",
        "            elif i == layers-1:\n",
        "                for j in range(output):\n",
        "                    z[i][j] = np.dot(a[i-1],weights[i][j]) + bias[i][j]           \n",
        "                a[i] = softmax(z[i])\n",
        "            else: \n",
        "                for j in range(node_per_layer):\n",
        "                    z[i][j] = np.dot(a[i-1],weights[i][j]) + bias[i][j]\n",
        "                    a[i][j] = relu(z[i][j])\n",
        "        value = a[layers-1]\n",
        "        # if testoutput[item].index(np.amax(testoutput[item])) == value.index(np.amax(value)):\n",
        "        #    correct += 1\n",
        "        if np.linalg.norm(testoutput[item]-value) < 0.01: # testing accuracy!\n",
        "            correct += 1\n",
        " \n",
        "        # printing the last 5 model output and actual output!\n",
        "        if outcounter == 79:\n",
        "            print('Printing last 5 Network output and test output:')\n",
        "        if outcounter > 79 :\n",
        "            print(value,testoutput[item])\n",
        "    print(\"Testing is now complete.\")\n",
        "    print(\"Accuracy of the model is :\", round(100 * correct/len(testinput),3)  ,\"%\")"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_ChTRKy82di"
      },
      "source": [
        "# Results\n",
        "We train the model with 4 hidden layers and 20 nodes per layer, with batch size 10, and learning rate 0.01.\n",
        "The network predicted mesothelioma with a accuracy close to 98%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufS1wC7jhrHC",
        "outputId": "cff93436-2b1f-49fe-e0a0-8ffff5587387"
      },
      "source": [
        "NeuralNet(traininput,trainoutput,testinput,testoutput,5,20,2,10,0.01)"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration  40 is now complete\n",
            "Iteration  80 is now complete\n",
            "Iteration  120 is now complete\n",
            "Iteration  160 is now complete\n",
            "Iteration  200 is now complete\n",
            "Training is now complete. It took 1.95 minutes to train the Network.\n",
            "Printing last 5 Network output and test output:\n",
            "[9.99869254e-01 1.30745995e-04] [1 0]\n",
            "[9.99946103e-01 5.38967330e-05] [1 0]\n",
            "[9.99982177e-01 1.78225969e-05] [1 0]\n",
            "[9.99090496e-01 9.09504092e-04] [1 0]\n",
            "[9.99988361e-01 1.16385494e-05] [1 0]\n",
            "Testing is now complete.\n",
            "Accuracy of the model is : 97.619 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}