{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural Network-1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Pho4xVBhtj"
      },
      "source": [
        "# Mushroom data set\n",
        "\n",
        "The Neural network was run on the mushroom data set and the goal was to classify the mushroom as poisonous or edible. The data set has 22 input values and since all of the were catogorical variable, it was converted to numerical variable using the get_dummies method in the pandas library, this converted the data set into a data set with 118 columns. \n",
        "\n",
        "It took a lot of time of time to figure out what initial weights to input, what activation functions to use, how to traing the data( whether in batches or not) etc. After some experimenting I decided to initialize the weights with a normal distribution with mean 0 and standard deviation that depends on the dimension of the data. \n",
        "\n",
        "In this Neural network, I used the relu activation function for the hidden layers and the softmax function for the output layer. The model was trained with a batch size of 20, and then updating the weights (This was giving good results).  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzYvY5fKT42F"
      },
      "source": [
        "# loading all the required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math \n",
        "import copy\n",
        "import pickle\n",
        "import time"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxC5tT6Mow8D"
      },
      "source": [
        "#preparing data, The data is split into 7000 train samples and, 1124 test samples\n",
        "data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/agaricus-lepiota.data\", header = None)\n",
        "data.drop([12], axis = 1) # droping the column with missing values. \n",
        "data = pd.get_dummies(data, columns = data.columns)\n",
        "datainput = data.iloc[:,2:]\n",
        "dataoutput = data.iloc[:,[0,1]]\n",
        "traininput = datainput.iloc[:7000,:].values\n",
        "trainoutput = dataoutput.iloc[:7000,:].values\n",
        "testinput = datainput.iloc[7000:,:].values\n",
        "testoutput = dataoutput.iloc[7000:,:].values"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAlpHXfgVRSX"
      },
      "source": [
        "# These are all the activation functions that I have trialed and errored with to make the model run.\n",
        "def relu(x):\n",
        "    if x > 0:\n",
        "        return x\n",
        "    else:\n",
        "        return 0\n",
        " \n",
        "def drelu(x):\n",
        "    if x > 0:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        " \n",
        "def sigmoid(x):\n",
        "    return 1/(1+math.exp(-x))\n",
        " \n",
        "def dsigmoid(x):\n",
        "    return sigmoid(x)*(1-sigmoid(x))\n",
        " \n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        " \n",
        "def dsoftmax(x):\n",
        "    return (x * np.identity(x.size)- x.transpose() @ x)\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMOtPYiuUDi-"
      },
      "source": [
        "# I started by trying to modularize everything, but ended up writing a single function that, trains and tests\n",
        "# the loaded data. \n",
        " \n",
        "# Parameters:\n",
        "# datainput - the training data\n",
        "# dataoutput - training data output\n",
        "# testinput - test data input\n",
        "# testoutput - test data output\n",
        "# layers - the number of layes in the network, this includes the number of hidden layers plus output layer.\n",
        "# node_per_layer - the number of nodes in a layer,\n",
        "# output - dimension of output/ number of classes. \n",
        " \n",
        " \n",
        "def NeuralNet(datainput, dataoutput, testinput, testoutput, layers, node_per_layer, output):\n",
        "\n",
        "    begin = time.time() # this is to calculate the runtime. \n",
        "    # Setting up all initial weights and biases, I have set them up as dictionaries\n",
        "    weights = {}\n",
        "    bias = {}\n",
        "    dweights = {}\n",
        "    dbias = {}\n",
        "    # These are set of dummy variables which I want in the shape of the weights, and their derivatives.\n",
        "    cweights = {}\n",
        "    cbias = {}\n",
        "    rweights = {}\n",
        "    rbias = {}\n",
        "    dim = len(datainput[0])\n",
        "    # Setting up initial weights and biases\n",
        "    for i in range(layers):\n",
        "        if i == 0:\n",
        "            weights[i] = np.random.normal(0,(2/dim)**(0.5),size = (node_per_layer,dim))\n",
        "            bias[i] = np.random.normal(0,1, size = (node_per_layer))\n",
        "            dweights[i] = np.zeros((node_per_layer,dim))\n",
        "            dbias[i] = np.zeros((node_per_layer))\n",
        "            cweights[i] = np.zeros((node_per_layer,dim))\n",
        "            cbias[i] = np.zeros((node_per_layer))\n",
        "            rweights[i] = np.zeros((node_per_layer,dim))\n",
        "            rbias[i] = np.zeros((node_per_layer))\n",
        "        elif i == layers-1:\n",
        "            weights[i] = np.random.uniform(-1,1,size = (output,node_per_layer))\n",
        "            bias[i] = np.random.uniform(-1/2,1/2,size = (output))\n",
        "            dweights[i] = np.random.uniform(1,5,size = (output,node_per_layer))\n",
        "            dbias[i] = np.random.uniform(1,4,size = output)\n",
        "            cweights[i] = np.zeros((output,node_per_layer))\n",
        "            cbias[i] = np.zeros((output))\n",
        "            rweights[i] = np.zeros((output,node_per_layer))\n",
        "            rbias[i] = np.zeros((output))\n",
        "        else:\n",
        "            weights[i] = np.random.uniform(-1,1,size = (node_per_layer,node_per_layer))\n",
        "            bias[i] = np.random.uniform(-1/2,1/2, size = (node_per_layer))\n",
        "            dweights[i] = np.random.uniform(1,5,size = (node_per_layer,node_per_layer))\n",
        "            dbias[i] = np.random.uniform(1,4,size = node_per_layer)\n",
        "            cweights[i] = np.zeros((node_per_layer,node_per_layer))\n",
        "            cbias[i] = np.zeros((node_per_layer))\n",
        "            rweights[i] = np.zeros((node_per_layer,node_per_layer))\n",
        "            rbias[i] = np.zeros((node_per_layer))\n",
        "    \n",
        " \n",
        "    a = {} #activation\n",
        "    z = {} #dummy to find activation using relu and softmax output.\n",
        "    delta = {} # catches the derivative of cost with respect to the dummy activation variables.\n",
        "    for i in range(layers):\n",
        "        if i == layers-1:\n",
        "            a[i] = np.empty(output)\n",
        "            z[i] = np.empty(output)\n",
        "            delta[i] = np.empty(output)\n",
        "        else:\n",
        "            a[i] = np.empty(node_per_layer)\n",
        "            z[i] = np.empty(node_per_layer)\n",
        "            delta[i] = np.empty(node_per_layer)\n",
        " \n",
        "    count = 0\n",
        "    # Iteration through the whole data set.\n",
        "    while count < 30:\n",
        "        flag = 0\n",
        "        itemcount = 0\n",
        "        for item in range(len(datainput)): # these are inputs from the data set.\n",
        " \n",
        "            # Calculating forwardpass with weights!\n",
        "            for i in range(layers):\n",
        "                if i == 0:\n",
        "                    for j in range(node_per_layer):\n",
        "                        z[i][j] = np.dot(datainput[item],weights[i][j]) + bias[i][j]\n",
        "                        a[i][j] = relu(z[i][j])\n",
        "                elif i == layers-1:\n",
        "                    for j in range(output):\n",
        "                        z[i][j] = np.dot(a[i-1],weights[i][j]) + bias[i][j]        \n",
        "                    a[i] = softmax(z[i])\n",
        "                else: \n",
        "                    for j in range(node_per_layer):\n",
        "                        z[i][j] = np.dot(a[i-1],weights[i][j]) + bias[i][j]\n",
        "                        a[i][j] = relu(z[i][j])\n",
        " \n",
        "            value = a[layers-1] #this is the forwardpass output.\n",
        "            dcost = np.empty((output)) \n",
        "            dvalue = dsoftmax(value) # the softmax deriative output\n",
        " \n",
        " \n",
        "            # Backpropagation! \n",
        " \n",
        "            # We use mean square error as our cost function. \n",
        "            # try to capture the derivatives in the same dictionary form as the weights, biases and the dummy z variables. \n",
        "            # Derivative of cost with respect to the forward pass output.\n",
        "            for i in range(output):\n",
        "                dcost[i] = -(dataoutput[item][i] - value[i])\n",
        " \n",
        "            # Finding the last layer gradients, all the gradient before this can be found from this. \n",
        "            for i in range(output):\n",
        "                delta[layers-1][i] = np.dot(dcost,dvalue[:,i]) # derivative with respect to the dummy z -variable\n",
        "                #delta[layers-1][i] = -2*(dataoutput[item][i]-value[i])*dsigmoid(z[layers-1][i])\n",
        "                dbias[layers-1][i] =  delta[layers-1][i]\n",
        "                for j in range(node_per_layer):\n",
        "                    dweights[layers-1][i][j] = delta[layers-1][i]*a[layers-2][j]\n",
        " \n",
        "            # Finding the gradient in the rest of the layers\n",
        "            for i in range(layers-1):\n",
        "                if i == layers-2:\n",
        "                    for j in range(node_per_layer):\n",
        "                        delta[layers-2-i][j] = np.dot(delta[layers-1-i],weights[layers-1-i][:,j])*drelu(z[layers-2-i][j])\n",
        "                        dbias[layers-2-i][j] = delta[layers-2-i][j]\n",
        "                        for k in range(dim):\n",
        "                            dweights[layers-2-i][j][k] = delta[layers-2-i][j] * datainput[item][k]\n",
        "                else:\n",
        "                    for j in range(node_per_layer):\n",
        "                        delta[layers-2-i][j] = np.dot(delta[layers-1-i],weights[layers-1-i][:,j])*drelu(z[layers-2-i][j])\n",
        "                        dbias[layers-2-i][j] = delta[layers-2-i][j]\n",
        "                        for k in range(node_per_layer):\n",
        "                            dweights[layers-2-i][j][k] = delta[layers-2-i][j]*a[layers-3-i][k]\n",
        " \n",
        " \n",
        "            # We train the model with batch size 20. i.e. find average gradient for 20 inputs and update the weights and biases\n",
        "            for i in range(layers):\n",
        "                cweights[i] = cweights[i] + dweights[i]\n",
        "                cbias[i] = cbias[i] + dbias[i]\n",
        "            \n",
        "            # updating weights in batches of size 20 - This was found to give meainingful results as compared to updating weights after and epoch.\n",
        "            # the 0.05 is the learning rate.\n",
        "            if flag > 0 and flag % 20 == 0:\n",
        "                for i in range(layers):\n",
        "                    weights[i] = weights[i] -  0.01 * cweights[i]/20 # the weights are being updated\n",
        "                    bias[i] = bias[i] -  0.01 * cbias[i]/20\n",
        "                    cweights[i] = rweights[i]\n",
        "                    cbias[i] = rbias[i]    \n",
        "            \n",
        "            flag += 1 \n",
        " \n",
        "            #if itemcount < 6:\n",
        "            #   print(value)\n",
        "        \n",
        " \n",
        "        count += 1 # counts epochs\n",
        "        if count % 5 == 0:\n",
        "            print(\"Iteration \", count, \"is now complete\")\n",
        "    end1 = time.time()\n",
        "    parameters = (weights, bias) # trained model weight!\n",
        "    pickle.dump(parameters, open('sample_data/parameters1.p', 'wb')) # dumping the trained model weights and biases\n",
        "    print(\"Training is now complete. It took\", round((end1-begin)/60,2) ,\"minutes to train the Network.\")\n",
        " \n",
        "    # Testing the model on untrained data.\n",
        "    # Now we test for accuracy.\n",
        "    correct = 0\n",
        "    outcounter = 0\n",
        "    for item in range(len(testinput)):\n",
        "        outcounter +=1\n",
        " \n",
        "        a = {} #activation\n",
        "        z = {} #dummy to find activation using relu.\n",
        "        for i in range(layers):\n",
        "            if i == layers-1:\n",
        "                a[i] = np.empty(output)\n",
        "                z[i] = np.empty(output)\n",
        "            else:\n",
        "                a[i] = np.empty(node_per_layer)\n",
        "                z[i] = np.empty(node_per_layer)\n",
        " \n",
        "        # Calculating forwardpass with weights!\n",
        "        for i in range(layers):\n",
        "            if i == 0:\n",
        "                for j in range(node_per_layer):\n",
        "                    z[i][j] = np.dot(testinput[item],weights[i][j]) + bias[i][j]\n",
        "                    a[i][j] = relu(z[i][j])\n",
        "            elif i == layers-1:\n",
        "                for j in range(output):\n",
        "                    z[i][j] = np.dot(a[i-1],weights[i][j]) + bias[i][j]           \n",
        "                a[i] = softmax(z[i])\n",
        "            else: \n",
        "                for j in range(node_per_layer):\n",
        "                    z[i][j] = np.dot(a[i-1],weights[i][j]) + bias[i][j]\n",
        "                    a[i][j] = relu(z[i][j])\n",
        "        value = a[layers-1]\n",
        "        # if testoutput[item].index(np.amax(testoutput[item])) == value.index(np.amax(value)):\n",
        "        #    correct += 1\n",
        "        if np.linalg.norm(testoutput[item]-value) < 0.02: # testing accuracy!\n",
        "            correct += 1\n",
        " \n",
        "        # printing the last 5 model output and actual output!\n",
        "        if outcounter == 1119:\n",
        "            print('Printing last 5 Network output and test output:')\n",
        "        if outcounter > 1119:\n",
        "            print(value,testoutput[item])\n",
        "    print(\"Testing is now complete.\")\n",
        "    print(\"Accuracy of the model is :\", round(100 * correct/len(testinput),3)  ,\"%\")"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWDOqFeRJLNH"
      },
      "source": [
        "# Results\n",
        "\n",
        "Here we have used 3 hidden layers and 20 nodes per layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NWF5Tu4qMip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33ff64c8-e994-467c-c092-aeeb9088cdfa"
      },
      "source": [
        "NeuralNet(traininput,trainoutput,testinput,testoutput,4,20,2)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration  5 is now complete\n",
            "Iteration  10 is now complete\n",
            "Iteration  15 is now complete\n",
            "Iteration  20 is now complete\n",
            "Iteration  25 is now complete\n",
            "Iteration  30 is now complete\n",
            "Training is now complete. It took 15.26 minutes to train the Network.\n",
            "Printing last 5 Network output and test output:\n",
            "[9.99999976e-01 2.41486759e-08] [1 0]\n",
            "[9.99999986e-01 1.38931807e-08] [1 0]\n",
            "[9.99999978e-01 2.23768093e-08] [1 0]\n",
            "[1.84999849e-04 9.99815000e-01] [0 1]\n",
            "[9.99996447e-01 3.55341939e-06] [1 0]\n",
            "Testing is now complete.\n",
            "Accuracy of the model is : 97.242 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}